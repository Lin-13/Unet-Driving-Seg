{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"yfhQ7eA28H1G"},"source":["## Transformer Segmentation for Autonomous Driving\n",">special thanks to [WX Lin](https://github.com/Lin-13)\n","\n","####   1. Download the data. \n","> It is provided in *driving_train_data.h5* and *driving_test_data.h5.* Implement a data loader class to handle the downloaded data. For more information on the dataset please refer to: [CityScapes dataset](https://www.cityscapes-dataset.com/)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2418,"status":"ok","timestamp":1676015354523,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"FkLjEFkF8H1H"},"outputs":[],"source":["import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","import torch\n","import h5py\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.transforms import ToTensor\n","import numpy as np\n","from tqdm import tqdm\n","traindata = h5py.File(\"./option1_driving-segmentation/driving_train_data.h5\")\n","testdata = h5py.File(\"./option1_driving-segmentation/driving_test_data.h5\")\n","# train_loader = data.DataLoader(dataset=traindata, batch_size=16, shuffle=True, pin_memory=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676015358941,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"zzWn6Fya8H1I"},"outputs":[],"source":["\"\"\"\n","refer to: https://blog.csdn.net/sinat_35779431/article/details/116991936\n","\"\"\"\n","\n","class MyDataset(data.Dataset):\n","    def __init__(self, archive,image='rgb',mask='seg',transform = None):\n","        # super().__init__()\n","        self.archive = h5py.File(archive, 'r')\n","        self.data = self.archive[image]\n","        self.labels = self.archive[mask]\n","        self.transform = transform\n","    def __getitem__(self, index):\n","        image = self.data[index]\n","        mask = self.labels[index]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            mask = torch.LongTensor(mask)\n","        return image, mask\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def close(self):\n","        self.archive.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676015363214,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"Pq0TKyVl8H1J"},"outputs":[],"source":["\n","train_dataset = MyDataset(\"./option1_driving-segmentation/driving_train_data.h5\",transform=ToTensor())\n","# data = data.ConcatDataset([rgb,seg])\n","test_dataset = MyDataset(\"./option1_driving-segmentation/driving_test_data.h5\",transform=ToTensor())\n","train_loader = data.DataLoader(dataset=train_dataset,batch_size=16,drop_last=True,shuffle=True)\n","test_loader = data.DataLoader(dataset=test_dataset,batch_size=16,drop_last=True,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"elapsed":2200,"status":"error","timestamp":1676015369942,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"148jhjuC8H1J","outputId":"dcb350d3-a331-4c2f-97d2-f420c6ab3d62"},"outputs":[],"source":["# show train images\n","rgb = traindata[\"rgb\"]\n","seg = traindata[\"seg\"]\n","plt.figure()\n","plt.subplot(1,2,1)\n","plt.imshow(rgb[0])\n","plt.title('color image')\n","plt.subplot(1,2,2)\n","seg[0].resize([2975,128,256,3])\n","plt.imshow(seg[0])\n","plt.title('segmentation map')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dMMXl4wl8H1K"},"source":["####   2. Define the model.\n","> Origin code from [CSDN](https://blog.csdn.net/weixin_44510615/article/details/119765864)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":860,"status":"ok","timestamp":1676015391279,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"gn5hTAEu8H1K"},"outputs":[],"source":["class UNet(nn.Module):\n","    def contracting_block(self, in_channels, out_channels, kernel_size=3, padding=1):\n","        block = torch.nn.Sequential(\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(out_channels),\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=out_channels, out_channels=out_channels, padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(out_channels),\n","                )\n","        return block\n","    \n","    def expansive_block(self, in_channels, mid_channel, out_channels, kernel_size=3, padding=1):\n","            block = torch.nn.Sequential(\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channel,padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(mid_channel),\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=mid_channel,padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(mid_channel),\n","                    torch.nn.ConvTranspose2d(in_channels=mid_channel, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n","                    )\n","            return  block\n","    \n","    def final_block(self, in_channels, mid_channel, out_channels, kernel_size=3, padding=1):\n","            block = torch.nn.Sequential(\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channel,padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(mid_channel),\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=mid_channel,padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(mid_channel),\n","                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channel, out_channels=out_channels, padding=padding),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm2d(out_channels),\n","                    )\n","            return  block\n","    \n","    def __init__(self, in_channel, out_channel):\n","        super(UNet, self).__init__()\n","        #Encode\n","        self.conv_encode1 = self.contracting_block(in_channels=in_channel, out_channels=64)\n","        self.conv_maxpool1 = torch.nn.MaxPool2d(kernel_size=2)\n","        self.conv_encode2 = self.contracting_block(64, 128)\n","        self.conv_maxpool2 = torch.nn.MaxPool2d(kernel_size=2)\n","        self.conv_encode3 = self.contracting_block(128, 256)\n","        self.conv_maxpool3 = torch.nn.MaxPool2d(kernel_size=2)\n","        # Bottleneck\n","        self.bottleneck = torch.nn.Sequential(\n","                            torch.nn.Conv2d(kernel_size=3, in_channels=256, out_channels=512,padding=1),\n","                            torch.nn.ReLU(),\n","                            torch.nn.BatchNorm2d(512),\n","                            torch.nn.Conv2d(kernel_size=3, in_channels=512, out_channels=512,padding=1),\n","                            torch.nn.ReLU(),\n","                            torch.nn.BatchNorm2d(512),\n","                            torch.nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n","                            )\n","        # Decode\n","        self.conv_decode3 = self.expansive_block(512, 256, 128)\n","        self.conv_decode2 = self.expansive_block(256, 128, 64)\n","        self.final_layer = self.final_block(128, 64, out_channel)\n","        \n","    def crop_and_concat(self, upsampled, bypass, crop=False):\n","        if crop:\n","            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n","            bypass = F.pad(bypass, (-c, -c, -c, -c))\n","        return torch.cat((upsampled, bypass), 1)\n","    \n","    def forward(self, x):\n","        # Encode\n","        encode_block1 = self.conv_encode1(x)\n","        encode_pool1 = self.conv_maxpool1(encode_block1)\n","        # print(\"encode1 output:\",encode_pool1.shape)\n","        encode_block2 = self.conv_encode2(encode_pool1)\n","        encode_pool2 = self.conv_maxpool2(encode_block2)\n","        # print(\"encode2 output:\",encode_pool2.shape)\n","        encode_block3 = self.conv_encode3(encode_pool2)\n","        encode_pool3 = self.conv_maxpool3(encode_block3)\n","        # print(\"encode3 output:\",encode_pool3.shape)\n","        # Bottleneck\n","        bottleneck1 = self.bottleneck(encode_pool3)\n","        # print(\"bottleneck1 output:\",bottleneck1.shape)\n","        # Decode\n","        decode_block3 = self.crop_and_concat(bottleneck1, encode_block3, crop=True)\n","        # print(\"decoder3 output:\",decode_block3.shape)\n","        cat_layer2 = self.conv_decode3(decode_block3)\n","        decode_block2 = self.crop_and_concat(cat_layer2, encode_block2, crop=True)\n","        # print(\"decoder2 output:\",decode_block2.shape)\n","        cat_layer1 = self.conv_decode2(decode_block2)\n","        decode_block1 = self.crop_and_concat(cat_layer1, encode_block1, crop=True)\n","        # print(\"decoder1 output:\",decode_block1.shape)\n","        final_layer = self.final_layer(decode_block1)\n","        # print(\"final output:\",final_layer.shape)\n","        return  final_layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SegmentationMetric(object):\n","    def __init__(self, numClass):\n","        self.numClass = numClass\n","        self.confusionMatrix = torch.zeros((self.numClass,)*2)\n"," \n","    def pixelAccuracy(self):\n","        # return all class overall pixel accuracy\n","        #  PA = acc = (TP + TN) / (TP + TN + FP + TN)\n","        acc = torch.diag(self.confusionMatrix).sum() /  self.confusionMatrix.sum()\n","        return acc\n"," \n","    def classPixelAccuracy(self):\n","        # return each category pixel accuracy(A more accurate way to call it precision)\n","        # acc = (TP) / TP + FP\n","        classAcc = torch.diag(self.confusionMatrix) / self.confusionMatrix.sum(axis=1)\n","        return classAcc # 返回的是一个列表值，如：[0.90, 0.80, 0.96]，表示类别1 2 3各类别的预测准确率\n"," \n","    def meanPixelAccuracy(self):\n","        classAcc = self.classPixelAccuracy()\n","        meanAcc = torch.nanmean(classAcc) # np.nanmean 求平均值，nan表示遇到Nan类型，其值取为0\n","        return meanAcc # 返回单个值，如：np.nanmean([0.90, 0.80, 0.96, nan, nan]) = (0.90 + 0.80 + 0.96） / 3 =  0.89\n"," \n","    def meanIntersectionOverUnion(self):\n","        # Intersection = TP Union = TP + FP + FN\n","        # IoU = TP / (TP + FP + FN)\n","        intersection = torch.diag(self.confusionMatrix) \n","        # 取对角元素的值，返回列表\n","        union = torch.sum(self.confusionMatrix, axis=1) + torch.sum(self.confusionMatrix, axis=0) - torch.diag(self.confusionMatrix) \n","        # axis = 1表示混淆矩阵行的值，返回列表； axis = 0表示取混淆矩阵列的值，返回列表 \n","        IoU = intersection / union  \n","        # 返回列表，其值为各个类别的IoU\n","        mIoU = torch.nanmean(IoU) \n","        # 求各类别IoU的平均\n","        return mIoU\n"," \n","    def genConfusionMatrix(self, imgPredict, imgLabel): # 同FCN中score.py的fast_hist()函数\n","        # remove classes from unlabeled pixels in gt image and predict\n","        mask = (imgLabel >= 0) & (imgLabel < self.numClass)\n","        label = self.numClass * imgLabel[mask] + imgPredict[mask]\n","        count = torch.bincount(label, minlength=self.numClass**2)\n","        confusionMatrix = count.reshape(self.numClass, self.numClass)\n","        confusionMatrix = confusionMatrix.cpu()\n","        return confusionMatrix\n"," \n","    def Frequency_Weighted_Intersection_over_Union(self):\n","        # FWIOU =     [(TP+FN)/(TP+FP+TN+FN)] *[TP / (TP + FP + FN)]\n","        freq = torch.sum(self.confusion_matrix, axis=1) / torch.sum(self.confusion_matrix)\n","        iu = torch.diag(self.confusion_matrix) / (\n","                torch.sum(self.confusion_matrix, axis=1) + torch.sum(self.confusion_matrix, axis=0) -\n","                torch.diag(self.confusion_matrix))\n","        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n","        return FWIoU\n"," \n"," \n","    def addBatch(self, imgPredict, imgLabel):\n","        assert imgPredict.shape == imgLabel.shape\n","        self.confusionMatrix += self.genConfusionMatrix(imgPredict, imgLabel)\n"," \n","    def reset(self):\n","        self.confusionMatrix = torch.zeros((self.numClass, self.numClass))\n","class KFMean():\n","    def __init__(self):\n","        self.acc_data  = 0\n","        self.count = 0\n","    def addData(self,data_append):\n","        self.count +=1\n","        self.acc_data = self.acc_data * (self.count - 1) / self.count + data_append /self.count\n","class Lut():\n","    def __init__(self,lut):\n","        self.lut = lut\n","    def transform(self,input:np.ndarray):\n","        if len(input.shape)!=2:\n","            raise Exception(\"dim Error!\")\n","        output = np.zeros(input.shape)\n","        output = self.lut[input]\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676015395068,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"F6vebZ8p8H1L","outputId":"e25952c3-f5cb-438e-b0f1-e95851af9753"},"outputs":[],"source":["# Visualizing the Model\n","unet = UNet(in_channel=3,out_channel=30)"]},{"cell_type":"markdown","metadata":{"id":"_jegFM198H1L"},"source":["#### 3. Define the loss function and optimizer. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6949,"status":"ok","timestamp":1676015408596,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"XrN1Zm_j8H1L","outputId":"7d35f2aa-ee7d-45a5-e39b-26e889e15979"},"outputs":[],"source":["#unet = UNet(in_channel=3,out_channel=30)\n","#out_channel represents number of segments desired\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(unet.parameters(), lr = 0.01, momentum=0.99)\n","optimizer.zero_grad()       \n","# input = torch.permute(torch.tensor(rgb[0][None],dtype=torch.float),(0,3,1,2))\n","# label = torch.tensor(seg[0],dtype = torch.long).permute(2,0,1)\n","input = torch.permute(torch.tensor(rgb[0][None],dtype=torch.float),(0,3,1,2))\n","label = torch.tensor(seg[0],dtype = torch.long).permute(2,0,1)\n","outputs = unet(input)\n","m = outputs.shape[0]\n","loss = criterion(outputs, label)\n","loss.backward()\n","optimizer.step()\n","input.shape,label.shape\n"]},{"cell_type":"markdown","metadata":{"id":"gWMhgksU8H1L"},"source":["#### 4. Train the network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296386,"status":"ok","timestamp":1676015709082,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"5qkSwvz78H1L","outputId":"7b0f1aaa-d464-4bbc-afb3-fdd247c3216f"},"outputs":[],"source":["unet = UNet(in_channel=3,out_channel=34)\n","# unet.load_state_dict(torch.load(\"driving-seg-epoch8.pt\"))\n","device = \"cuda:0\"\n","print(torch.cuda.is_available())\n","unet.to(device=device)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(unet.parameters(), lr = 0.01, momentum=0.99)\n","num_epoch = 30\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["losses = np.zeros((num_epoch,len(train_loader)))\n","mIoUs = np.zeros(num_epoch)\n","for i in range(num_epoch):\n","    metric = SegmentationMetric(34)\n","    bar = tqdm(range(len(train_loader)))\n","    lossAccmulator = []\n","    for j,(input,label) in enumerate(train_loader):\n","        input = input.to(device)\n","        label = label.to(device)\n","        optimizer.zero_grad()\n","        label = torch.permute(label,(3,0,1,2))[0]\n","        outputs = unet(input)\n","        loss = criterion(outputs, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        losses[i][j] =  loss.detach().cpu().numpy()\n","        _,predicted = torch.max(outputs,1)\n","        imgPredict = predicted.squeeze()\n","        imgLabel = label.squeeze()\n","        metric.addBatch(imgPredict, imgLabel)\n","        bar.update()\n","\n","    mIoU = metric.meanIntersectionOverUnion()\n","    mIoUs[i] = mIoU.cpu().numpy()\n","    print(\"epoch {}: mIoU is : {}.\\n\".format(i,mIoU))\n","    bar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(unet.state_dict(),\"driving-seg-epoch30.pt\")\n","torch.save(unet,\"driving-seg-30.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(losses.T)\n","plt.legend([\"epoch {}\".format(i) for i in range(num_epoch)])"]},{"cell_type":"markdown","metadata":{"id":"r-6noYzQ8H1M"},"source":["#### 5. Test the resulting network on examples from an independent test set.\n","\n","Implement and present:  \n","    a. Create and save a bar chart showing the test accuracy across each semantic class.  \n","    b. Predictions and visualizations for (𝑖𝑛𝑝𝑢𝑡, µ, 𝑎𝑙𝑒𝑎𝑡𝑜𝑟𝑖𝑐, 𝑒𝑝𝑖𝑠𝑡𝑒𝑚𝑖𝑐) on 5 different input examples. Please save and visualize the results on each of the 5 different inputs. Since there are 4 visualizations per input, please provide a total of 5*4 = 20 images.  \n","    c. Comment briefly on how the model’s performance could be improved.  \n","    d. Please save your code and results for submission.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":2487,"status":"ok","timestamp":1676017080972,"user":{"displayName":"John Zhao","userId":"15079769279491459053"},"user_tz":-480},"id":"W_ox6qbb8H1M","outputId":"ddb4440a-76c5-4c02-e402-7200b519f620"},"outputs":[],"source":["# Comparing the Results\n","# unet = UNet(in_channel=3,out_channel=34)\n","# unet.load_state_dict(torch.load(\"driving-seg-epoch30.pt\"))\n","unet.eval()\n","unet.to(device)\n","images0, labels0 =  test_dataset[0]\n","images0 = images0[None]\n","labels0 = labels0[None]\n","labels0 = torch.permute(labels0,(3,0,1,2))[0]\n","images0 = images0.to(device)\n","\n","outputs0 = unet(images0)\n","outputs0 = outputs0.cpu()\n","_, predicted = torch.max(outputs0, 1)\n","plt.subplot(211)\n","plt.imshow(predicted[0])\n","plt.subplot(212)\n","plt.imshow(labels0[0])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\"\n","# unet = UNet(in_channel=3,out_channel=34)\n","# unet.load_state_dict(torch.load(\"driving-seg.pt\"))\n","unet.to(device)\n","# unet = torch.load(\"driving-seg-unet.pt\")\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","unet.eval()\n","losses_test = np.zeros((len(test_loader)))\n","\n","metric = SegmentationMetric(34)\n","bar = tqdm(range(len(test_loader)))\n","lossAccmulator = []\n","with torch.no_grad():\n","    for i,(input,label) in enumerate(test_loader):\n","        input = input.to(device)\n","        label = torch.permute(label,(3,0,1,2))[0]\n","        label = label.to(device)\n","    \n","        outputs = unet(input)\n","        loss = criterion(outputs, label)\n","        \n","        losses_test[i] =  loss.detach().cpu().numpy()\n","        _,predicted = torch.max(outputs,1)\n","        imgPredict = predicted.squeeze()\n","        imgLabel = label.squeeze()\n","        metric.addBatch(imgPredict, imgLabel)\n","        bar.update()\n","mIoU = metric.meanIntersectionOverUnion()\n","print(\"test: mIoU is :{}.\\n\".format(mIoU))\n","bar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(losses_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["losses_test.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(losses[0][:])\n","plt.plot(losses[-1][:])\n","plt.legend([\"epoch 1\",\"epoch {}\".format(num_epoch)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(losses,axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.save(\"losses-epoch30.npy\",losses)\n","np.save(\"mIoUs-epoch30.npy\",mIoUs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.mean(losses,axis = 1))\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(mIoUs)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"mIoU\")\n","print(\"test_dataset: mIoU=\",mIoU)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","class VideoLoader():\n","    def __init__(self,file:str,batch_size:int = 1,transform = ToTensor(),resize_shape :tuple = None):\n","        self.cap = cv2.VideoCapture(file)\n","        self.batch_size = batch_size\n","        self.idx = 0\n","        self.transform = transform\n","        if resize_shape is not None:\n","            self.resized_shape = resize_shape\n","        else:\n","            self.resized_shape = self.frameshape()\n","        if not self.cap.isOpened():\n","            raise Exception(\"file error\")\n","    def __iter__(self):\n","        return self\n","    def __next__(self):\n","        if self.idx >= int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n","            self.idx = 0\n","            raise StopIteration\n","        if self.cap.get(cv2.CAP_PROP_FRAME_COUNT) -self.idx < self.batch_size:\n","            size  = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) -self.idx)\n","        else:\n","            size = self.batch_size\n","        next = torch.zeros((size,\n","                            3,\n","                        #  int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n","                        #  int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","                            *self.resized_shape\n","                        ),dtype=torch.float)\n","        for i in range(size):\n","            success,frame = self.cap.read()\n","            if self.resized_shape is not None:\n","                frame = cv2.resize(frame,(self.resized_shape[1],self.resized_shape[0]))\n","            if success:\n","                if self.transform is not None:\n","                    frame = self.transform(frame)\n","                next[i] = frame\n","            pass\n","        self.idx +=self.batch_size\n","        return next\n","    def frameshape(self):\n","        return (int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n","                int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)))\n","    def fps(self):\n","        return self.cap.get(cv2.CAP_PROP_FPS)\n","    def __len__(self):\n","        return int(np.ceil(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)/self.batch_size))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["video_batch = 32\n","video = VideoLoader(\"images/test.mp4\",video_batch,resize_shape=(128,256))\n","# video = VideoLoader(\"images/test.mp4\",video_batch)\n","video.cap.get(cv2.CAP_PROP_FRAME_COUNT),len(video),video.resized_shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bar = tqdm(range(len(video)))\n","device = \"cuda\"\n","unet = torch.load(\"driving-seg-30.pt\")\n","unet.to(device)\n","unet.eval()\n","# video output\n","video_write = cv2.VideoWriter(\"images/test128p.avi\",\n","                              cv2.VideoWriter_fourcc('M','J','P','G'), \n","                              video.fps(), (video.resized_shape[1],video.resized_shape[0]),\n","                              True)\n","with torch.no_grad():\n","    for i,data in  enumerate(video):\n","        data = data.to(device)\n","        outputs = unet(data)\n","        _,predicted = torch.max(outputs,1)\n","        preds = predicted.detach().cpu().numpy()\n","        # print(\"data: {}\\n\".format(data.shape))\n","        for j in range(preds.shape[0]):\n","            img = np.ones((*video.resized_shape,3),dtype=np.uint8)\n","            img[:,:,0]=preds[j]/50*255 + 50\n","            img[:,:,1]=180\n","            img[:,:,2]=180\n","            img = cv2.cvtColor(img,cv2.COLOR_HSV2BGR)\n","            video_write.write(img)\n","        bar.update()\n","    video_write.release()\n","    bar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lookup = Lut(np.array([2,4,6,8])) #1 dim\n","lookup = Lut(np.array([[2,1],[4,1],[6,1],[8,1]]))\n","lut_input = np.array([1,2,3,0]).reshape(2,2)\n","lut_output = lookup.transform(lut_input)\n","lut_output"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"py_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"0fc8d7919e9f10cde9ff99f855ad00ea2e279a58059c8fe289ad97ff953fd645"}}},"nbformat":4,"nbformat_minor":0}
